import streamlit as st
import logging
import ollama

from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from typing import List, Tuple, Dict, Any
from PyPDF2 import PdfReader

st.set_page_config(
    page_title="Ollama PDF RAG Streamlit UI",
    page_icon="ğŸˆ",
    layout="wide",
    initial_sidebar_state="collapsed",
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)


@st.cache_resource(show_spinner=True)
def extract_model_names(
        models_info: Dict[str, List[Dict[str, Any]]],
) -> Tuple[str, ...]:
    logger.info("Extracting model names from models_info")
    model_names = tuple(model["name"] for model in models_info["models"])
    logger.info(f"Extracted model names: {model_names}")
    return model_names


def get_pdf_texts(pdf_docs):
    texts = ''

    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            texts += page.extract_text()

    return texts


def create_vector_db(text) -> Chroma:
    text_splitter = CharacterTextSplitter(
        separator='\n',
        chunk_size=7500,
        chunk_overlap=100,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    logger.info("Document split into chunks")

    embeddings = OllamaEmbeddings(model="nomic-embed-text", show_progress=True)
    vector_db = Chroma.from_texts(
        texts=chunks, embedding=embeddings, collection_name="RAG"
    )

    return vector_db


def process_question(question: str, vector_db: Chroma, selected_model: str) -> str:
    logger.info(f"""Processing question: {question} using model: {selected_model}""")
    llm = ChatOllama(model=selected_model, temperature=0)
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""Báº¡n lÃ  trá»£ lÃ½ mÃ´ hÃ¬nh ngÃ´n ngá»¯ AI. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra 3
         cÃ¡c phiÃªn báº£n khÃ¡c nhau cá»§a cÃ¢u há»i ngÆ°á»i dÃ¹ng nháº¥t Ä‘á»‹nh Ä‘á»ƒ truy xuáº¥t cÃ¡c 
         tÃ i liá»‡u liÃªn quan tá»« vector database. Báº±ng cÃ¡ch táº¡o ra nhiá»u gÃ³c nhÃ¬n cho 
         cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng. Má»¥c tiÃªu lÃ  giÃºp ngÆ°á»i dÃ¹ng kháº¯c phá»¥c má»™t sá»‘ háº¡n cháº¿ 
         cá»§a viá»‡c truyá»n dá»¯ liá»‡u dá»±a trÃªn khoáº£ng cÃ¡ch tÃ¬m kiáº¿m tÆ°Æ¡ng tá»±. Cung cáº¥p cÃ¡c 
         cÃ¢u há»i thay tháº¿ nÃ y Ä‘Æ°á»£c phÃ¢n tÃ¡ch báº±ng dÃ²ng má»›i.
         CÃ¢u há»i gá»‘c: {question}""",
    )

    retriever = MultiQueryRetriever.from_llm(
        vector_db.as_retriever(), llm, prompt=QUERY_PROMPT
    )

    template = """Tráº£ lá»i cÃ¢u há»i dá»±a theo ná»™i dung cá»§a tÃ i liá»‡u sau:
    {context}
    CÃ¢u há»i: {question}
    Náº¿u báº¡n khÃ´ng biáº¿t vui lÃ²ng tráº£ lá»i má»™t cÃ¡ch lá»‹ch sá»­ lÃ  tÃ´i khÃ´ng biáº¿t, khÃ´ng Ä‘Æ°á»£c phÃ©p tá»± Ã½ bá»‹a Ä‘áº·t 
    Chá»‰ tráº£ lá»i cÃ¡c cÃ¢u há»i á»Ÿ trong {context}, khÃ´ng tráº£ lá»i á»Ÿ ngoÃ i ná»™i dung file.
    ThÃªm Ä‘oáº¡n ngá»¯ cáº£nh báº¡n Ä‘Ã£ sá»­ dá»¥ng Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Chá»‰ tráº£ lá»i báº±ng tiáº¿ng viá»‡t
    """

    prompt = ChatPromptTemplate.from_template(template)

    chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    response = chain.invoke(question)
    return response


def main() -> None:
    models_info = ollama.list()
    available_models = extract_model_names(models_info)

    col1, col2 = st.columns([1.5, 2])

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    if "vector_db" not in st.session_state:
        st.session_state["vector_db"] = None

    if available_models:
        selected_model = col2.selectbox(
            "Chá»n model â†“", available_models
        )

    file_upload = col1.file_uploader(
        "Chá»n file â†“", type="pdf", accept_multiple_files=True
    )

    if file_upload:
        st.session_state["file_upload"] = file_upload
        if st.session_state["vector_db"] is None:
            chunk_texts = get_pdf_texts(file_upload)
            st.session_state["vector_db"] = create_vector_db(chunk_texts)

    with col2:
        message_container = st.container(height=300, border=True)

        for message in st.session_state["messages"]:
            avatar = "ğŸ¤–" if message["role"] == "assistant" else "ğŸ˜"
            with message_container.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])

        if prompt := st.chat_input("Nháº­p vÃ o ná»™i dung..."):
            try:
                st.session_state["messages"].append({"role": "user", "content": prompt})
                message_container.chat_message("user", avatar="ğŸ˜").markdown(prompt)

                with message_container.chat_message("assistant", avatar="ğŸ¤–"):
                    with st.spinner(":green[Processing...]"):
                        if st.session_state["vector_db"] is not None:
                            response = process_question(
                                prompt, st.session_state["vector_db"], selected_model
                            )
                            st.markdown(response)
                        else:
                            st.warning("Vui lÃ²ng chá»n file.")

                if st.session_state["vector_db"] is not None:
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": response}
                    )

            except Exception as e:
                st.error(e, icon="â›”ï¸")
                logger.error(f"Error processing prompt: {e}")
        else:
            if st.session_state["vector_db"] is None:
                st.warning("Chá»n file Ä‘á»ƒ báº¯t Ä‘áº§u...")


if __name__ == "__main__":
    main()
